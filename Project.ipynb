{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.sql import functions as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Description\n",
    "This dataset contains roughly 20B tweets, from June 2013 to Jan 2016. The tweets are not filtered, and could be roughly 10% of tweets during that time. The format is unicode text with one line per tweet, including language, date and user. Only language detected by twitter as de,en,es,fr,it,nz are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "data = sc.textFile(\"/datasets/tweets-leon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1 Handle the data in its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'en\\t345963923251539968\\tSat Jun 15 18:00:01 +0000 2013\\tLetataleta\\tRT @silsilfani: the world is not a wish-granting machine. dont be surprised when everything always end up disappointing.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frist_tweet = data.first()\n",
    "frist_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Chose tweets that have exactly 5 components like normal \n",
    "         (language, id, date, username, content)\"\"\"\n",
    "\n",
    "def selection_tweet(tweet):\n",
    "    return len(tweet.split(\"\\t\")) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"encode tweet\"\n",
    "\n",
    "def encode_tweet(tweet):\n",
    "        return [t.encode(\"utf8\") for t in tweet.split(\"\\t\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en',\n",
       " '345963923251539968',\n",
       " 'Sat Jun 15 18:00:01 +0000 2013',\n",
       " 'Letataleta',\n",
       " 'RT @silsilfani: the world is not a wish-granting machine. dont be surprised when everything always end up disappointing.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_tweet(frist_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chose a sub dataset for working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.filter(selection_tweet)\n",
    "\n",
    "en_data = data.filter(lambda x : x[:2]=='en')\n",
    "de_data = data.filter(lambda x : x[:2]=='de')\n",
    "es_data = data.filter(lambda x : x[:2]=='es')\n",
    "fr_data = data.filter(lambda x : x[:2]=='fr')\n",
    "it_data = data.filter(lambda x : x[:2]=='it')\n",
    "nz_data = data.filter(lambda x : x[:2]=='nz')\n",
    "\n",
    "data_2013 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2013')\n",
    "data_2014 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2014')\n",
    "data_2015 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2015')\n",
    "data_2016 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'fr\\t345963923255730176\\tSat Jun 15 18:00:01 +0000 2013\\t_irem61_\\tRT @DHC_Music: Terrorism ... #FreePalestina http://t.co/OLWnVlW682',\n",
       " u'fr\\t345963923335413761\\tSat Jun 15 18:00:01 +0000 2013\\tHairCutGroup\\tHair by Unihair-boutique\\\\nhttp://t.co/fRGYMrRcQe http://t.co/7sXXzn1x34',\n",
       " u'fr\\t345963923465441280\\tSat Jun 15 18:00:01 +0000 2013\\tChajopico\\tRT @KimberleyNoe: \" J\\'ai rencontr\\xe9 un autre. On a pass\\xe9 des moments formidables. C\\'\\xe9tait bien mais ce n\\'\\xe9tait pas toi. \"',\n",
       " u'fr\\t345963923360604161\\tSat Jun 15 18:00:01 +0000 2013\\tyousinceforever\\tDu coup sa ma blaser de tourner en rond la!',\n",
       " u\"fr\\t345963923398328320\\tSat Jun 15 18:00:01 +0000 2013\\tGraciaDiamond_\\tLa meilleure de #SS7 c'est Ana\\xefs sans aucun doute!!! ;-)\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_fr_tweets = fr_data.take(5)\n",
    "some_fr_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2 Understand what’s into the data (formats, distributions, missing values, correlations, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_fr_tweets = [encode_tweet(tweet) for tweet in some_fr_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some french tweets:\n",
      "1 )User name: _irem61_\n",
      "         Tweets: RT @DHC_Music: Terrorism ... #FreePalestina http://t.co/OLWnVlW682\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "2 )User name: HairCutGroup\n",
      "         Tweets: Hair by Unihair-boutique\\nhttp://t.co/fRGYMrRcQe http://t.co/7sXXzn1x34\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "3 )User name: Chajopico\n",
      "         Tweets: RT @KimberleyNoe: \" J'ai rencontré un autre. On a passé des moments formidables. C'était bien mais ce n'était pas toi. \"\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "4 )User name: yousinceforever\n",
      "         Tweets: Du coup sa ma blaser de tourner en rond la!\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n",
      "5 )User name: GraciaDiamond_\n",
      "         Tweets: La meilleure de #SS7 c'est Anaïs sans aucun doute!!! ;-)\n",
      "         at: Sat Jun 15 18:00:01 +0000 2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Some french tweets:'\n",
    "for ind, t in enumerate(some_fr_tweets):\n",
    "    print ind + 1,')User name:',t[3]\n",
    "    print '         Tweets:', t[4]\n",
    "    print '         at:', t[2]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3 Considered ways to enrich, filter, transform the data according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|                id|            sentence|\n",
      "+------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|\n",
      "|345963923259924480|Can't stand peopl...|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Take only the data in english\"\"\"\n",
    "en_data = data.filter(lambda x : x[:2]=='en')\n",
    "\n",
    "\"\"\"Encode UTF-8\"\"\"\n",
    "en_data = en_data.map(encode_tweet)\n",
    "\n",
    "\"\"\"Take only ID and CONTENT of a tweet\"\"\"\n",
    "tweets = en_data.map(lambda tweet : Row(id=tweet[1], sentence=tweet[4]))\n",
    "\n",
    "\"\"\"Create DF\"\"\"\n",
    "df_tweets = sqlContext.createDataFrame(tweets)\n",
    "\n",
    "df_tweets.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|                id|            sentence|                 raw|\n",
      "+------------------+--------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|[rt, silsilfani, ...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|[rt, whosthishoe,...|\n",
      "|345963923259924480|Can't stand peopl...|[can, t, stand, p...|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenization\"\"\"\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"raw\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(df_tweets)\n",
    "\n",
    "regexTokenized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|                id|            sentence|                 raw|            filtered|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|[rt, silsilfani, ...|[rt, silsilfani, ...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|[rt, whosthishoe,...|[rt, whosthishoe,...|\n",
      "|345963923259924480|Can't stand peopl...|[can, t, stand, p...|[t, stand, people...|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Remove Stop-words\"\"\"\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "removed_stopwords = remover.transform(regexTokenized)\n",
    "\n",
    "removed_stopwords.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemmatization'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Lemmatization\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Warning: Computation of TF-IDF and LDA take a lot of time (1h30 on the cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TF-IDF\"\"\"\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "count_vectorizer_model = cv.fit(removed_stopwords)\n",
    "tf = count_vectorizer_model.transform(removed_stopwords)\n",
    "\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "tfidf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Topics extraction with LDA\"\"\"\n",
    "\n",
    "nbTopics=100\n",
    "n_terms=15\n",
    "\n",
    "corpus = tfidf.select(F.col('id').cast(\"long\"), 'tfidf').rdd.map(lambda x: [x[0], x[1]])\n",
    "ldaModel = LDA.train(corpus, k=nbTopics)\n",
    "\n",
    "\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=n_terms)\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "\n",
    "\"\"\"Store result\"\"\"\n",
    "with open(\"topics.pickle\", \"wb\") as f:\n",
    "    pickle.dump(topics, f)\n",
    "with open(\"vocabulary.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocabulary, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load result computed from cluster\"\"\"\n",
    "\n",
    "with open(\"topics.pickle\", \"rb\") as f:\n",
    "    topics = pickle.load(f)\n",
    "    \n",
    "with open(\"vocabulary.pickle\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for topic in range(len(topics)):\n",
    "    print(\"topic {} : \".format(topic))\n",
    "    words = topics[topic][0]\n",
    "    scores = topics[topic][1]\n",
    "    for word in range(len(words)):\n",
    "        print(vocabulary[words[word]], \"-\", scores[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4 Updated your plan in a reasonable way, reflecting your improved knowledge after data acquaintance. In particular, discuss how your data suits your project needs and discuss the methods you’re going to use, giving their essential mathematical details in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5 That your plan for analysis and communication is now reasonable and sound, potentially discussing alternatives to your choices that you considered but dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : \n",
      "(u'going', '-', 0.05409342650763188)\n",
      "(u'bed', '-', 0.05407417503427965)\n",
      "(u'want', '-', 0.05401486983908868)\n",
      "topic 1 : \n",
      "(u'java', '-', 0.05407759673070426)\n",
      "(u'classes', '-', 0.05390088196503407)\n",
      "(u'case', '-', 0.053459237515048524)\n",
      "topic 2 : \n",
      "(u'today', '-', 0.0532383963697219)\n",
      "(u'day', '-', 0.05306853182395553)\n",
      "(u'hi', '-', 0.052806852808286335)\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = sqlContext.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\"),\n",
    "    (3, \"I want a coffee before going to bed\"),\n",
    "    (4, \"Today is a big day !!!\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "removed = remover.transform(regexTokenized)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "count_vectorizer_model = cv.fit(removed)\n",
    "tf = count_vectorizer_model.transform(removed)\n",
    "\n",
    "\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "nbTopics=3\n",
    "n_terms=3\n",
    "\n",
    "corpus = tfidf.select(F.col('id').cast(\"long\"), 'tfidf').rdd.map(lambda x: [x[0], x[1]])\n",
    "ldaModel = LDA.train(corpus, k=nbTopics)\n",
    "\n",
    "\n",
    "# extracting topics\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=n_terms)\n",
    "# extraction vocabulary\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "file = open(\"testfile.txt\",\"w\")\n",
    "for topic in range(len(topics)):\n",
    "    print(\"topic {} : \".format(topic))\n",
    "    file.write(\"topic {} : \\n\".format(topic)) \n",
    "    words = topics[topic][0]\n",
    "    scores = topics[topic][1]\n",
    "    for word in range(len(words)):\n",
    "        file.write(\"{} - {}\\n\".format(vocabulary[words[word]], scores[word]))\n",
    "        print(vocabulary[words[word]], \"-\", scores[word])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
