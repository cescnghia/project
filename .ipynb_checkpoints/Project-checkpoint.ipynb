{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following trends on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Abstract](#Abstract)\n",
    "\n",
    "[2. Exploratory data analysis](#Exploratory-data-analysis)\n",
    "\n",
    "[3. Topic Modeling using LDA](#A-very-simple-Topic-Modeling-using-LDA)\n",
    "\n",
    "[4. Example of the pipeline](#Example-of-the-pipeline-that-we-will-follow-for-the-LDA-algorithm)\n",
    "\n",
    "[5. Milestone 3](#Milestone-3:-the-data-story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realiazed that it will be hard to achieve the goal stated in milestone 1(detecting fake news). The problem is that we couldn't find a way to define fake news. And also the twitter dataset is not what we expected it to be. For example it doesn't contain the number of times a tweet has been retweeted, the geographical location, number of likes ... So we decided to go in a different, more feasable direction, which is following the process of creating and spreading trends on Twitter. Trying to find patterns between trends and users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.sql import functions as F\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each data entry has 5 fields:\n",
    "     - language: language of the user \n",
    "     - id: id of the user\n",
    "     - date: date when the tweet was published\n",
    "     - username: username of the user\n",
    "     - content: the tweet\n",
    "     \n",
    "Given that for the moment we consider only rows that have all 5 fields we don't have to deal with missing values.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "data = sc.textFile(\"/datasets/tweets-leon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1 We will first clean the data and select only a subset that is useful for this project:\n",
    "    - keep only the tweets that have all 5 fields\n",
    "    - remove urls from the content\n",
    "    - remove emojis\n",
    "    - remove punctuation \n",
    "    - remove stopwords\n",
    "    - apply lemmatization \n",
    "    - keep only english, spanish and french tweets\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'en\\t345963923251539968\\tSat Jun 15 18:00:01 +0000 2013\\tLetataleta\\tRT @silsilfani: the world is not a wish-granting machine. dont be surprised when everything always end up disappointing.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frist_tweet = data.first()\n",
    "frist_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Chose tweets that have exactly 5 components like normal \n",
    "         (language, id, date, username, content)\"\"\"\n",
    "\n",
    "def selection_tweet(tweet):\n",
    "    contents = tweet.split(\"\\t\")\n",
    "    if (contents[0] == 'en'):\n",
    "        if (len(contents) == 5):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Remove punctuation\"\"\"\n",
    "\n",
    "table = string.maketrans(\"\",\"\")\n",
    "def punctuation(s):\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    return s.translate(table, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"encode tweet by mapping\"\n",
    "\n",
    "def encode_tweet(tweet):\n",
    "    \"Encode UTF-8\"\n",
    "    encoded = [t.encode(\"utf8\") for t in tweet.split(\"\\t\")]\n",
    "    \"Remove punctuation\"\n",
    "    encoded[4] = punctuation(encoded[4])\n",
    "    \"Remove 2-grams\"\n",
    "    encoded[4] = ' '.join([x for x in encoded[4].split(' ') if len(x) > 2])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en',\n",
       " '345963923251539968',\n",
       " 'Sat Jun 15 18:00:01 +0000 2013',\n",
       " 'Letataleta',\n",
       " 'silsilfani the world not wishgranting machine dont surprised when everything always end disappointing']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_tweet(frist_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on using the filter function as done below we will select only a useful subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"remove urls\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"remove emojis\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"remove punctuation\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Lemmatization\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"select a subset of the data\"\"\"\n",
    "\n",
    "data = data.filter(selection_tweet)\n",
    "\n",
    "en_data = data.filter(lambda x : x[:2]=='en')\n",
    "es_data = data.filter(lambda x : x[:2]=='es')\n",
    "fr_data = data.filter(lambda x : x[:2]=='fr')\n",
    "\n",
    "data_2012 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2012')\n",
    "data_2013 = data.filter(lambda tweet : \n",
    "                        encode_tweet(tweet)[2][-4:] == '2013')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_fr_tweets = fr_data.take(5)\n",
    "some_fr_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_fr_tweets = [encode_tweet(tweet) for tweet in some_fr_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Some french tweets:'\n",
    "for ind, t in enumerate(some_fr_tweets):\n",
    "    print ind + 1,')User name:',t[3]\n",
    "    print '         Tweets:', ((t[4]))\n",
    "    print '         at:', t[2]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very simple Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to familiarize ourselves with the dataset we started with a very simple approach for topic extraction. For achieving this we will use the Latent Dirichlet allocation algorithm. We first need to build the tf-idf matrix using our data and then then pass it as a parameter to the LDA method. We also need to specify the number of topics to be extracted from the dataset, α(parameter of the Dirichlet prior on the per-document topic distributions) and β(parameter of the Dirichlet prior on the per-topic word distribution). We will determine this values in the next milestone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Data in english and encode UTF-8\"\"\"\n",
    "en_data = data.filter(selection_tweet).map(encode_tweet)\n",
    "\n",
    "\"\"\"Take only ID and CONTENT of a tweet\"\"\"\n",
    "tweets = en_data.map(lambda tweet : Row(id=tweet[1], sentence=tweet[4]))\n",
    "\n",
    "\"\"\"Create DF\"\"\"\n",
    "df_tweets = sqlContext.createDataFrame(tweets)\n",
    "\n",
    "df_tweets.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|                id|            sentence|                 raw|\n",
      "+------------------+--------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|[rt, silsilfani, ...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|[rt, whosthishoe,...|\n",
      "|345963923259924480|Can't stand peopl...|[can, t, stand, p...|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenization\"\"\"\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"raw\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(df_tweets)\n",
    "\n",
    "regexTokenized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"regexTokenized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(regexTokenized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|                id|            sentence|                 raw|            filtered|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|345963923251539968|RT @silsilfani: t...|[rt, silsilfani, ...|[rt, silsilfani, ...|\n",
      "|345963923297673217|RT @WhosThisHoe: ...|[rt, whosthishoe,...|[rt, whosthishoe,...|\n",
      "|345963923259924480|Can't stand peopl...|[can, t, stand, p...|[t, stand, people...|\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Remove Stop-words\"\"\"\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "removed_stopwords = remover.transform(regexTokenized)\n",
    "with open(\"removed_stopwords.pickle\", \"wb\") as f:\n",
    "    pickle.dump(removed_stopwords, f)\n",
    "removed_stopwords.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"apply lemmatization and remove punctuation\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Warning: Computation of TF-IDF and LDA take a lot of time (6h on the cluster, we don't know what's happening, why it takes so long ? Could you take a look at our code to see if maybe there is a problem with it). This is the reason why we have an extra python file (LDA.py) to submit to the cluster for the computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"TF-IDF\"\"\"\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "count_vectorizer_model = cv.fit(removed_stopwords)\n",
    "tf = count_vectorizer_model.transform(removed_stopwords)\n",
    "\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "tfidf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Topics extraction with LDA\"\"\"\n",
    "\n",
    "nbTopics=100\n",
    "n_terms=15\n",
    "\n",
    "corpus = tfidf.select(F.col('id').cast(\"long\"), 'tfidf').rdd.map(lambda x: [x[0], x[1]])\n",
    "ldaModel = LDA.train(corpus, k=nbTopics)\n",
    "\n",
    "\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=n_terms)\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "\n",
    "\"\"\"Store result\"\"\"\n",
    "with open(\"topics.pickle\", \"wb\") as f:\n",
    "    pickle.dump(topics, f)\n",
    "with open(\"vocabulary.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocabulary, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load result computed from cluster\"\"\"\n",
    "\n",
    "with open(\"topics.pickle\", \"rb\") as f:\n",
    "    topics = pickle.load(f)\n",
    "    \n",
    "with open(\"vocabulary.pickle\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for topic in range(len(topics)):\n",
    "    print(\"topic {} : \".format(topic))\n",
    "    words = topics[topic][0]\n",
    "    scores = topics[topic][1]\n",
    "    for word in range(len(words)):\n",
    "        print(vocabulary[words[word]], \"-\", scores[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = sc.textFile(\"/user/khau/topics.pickle\")\n",
    "vocabulary = sc.textFile(\"/user/khau/vocabulary.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_topics(topics, vocabulary, nbTopics, n_terms):\n",
    "    topic_scores = [x[1] for x in topics] #all scores\n",
    "    max_score = np.max(topic_scores)\n",
    "    nbColsPlot = 4\n",
    "    MAGIC_NUMBER = 50\n",
    "    fontsize_init = MAGIC_NUMBER / max_score\n",
    "\n",
    "    for topic in range(len(topics)):\n",
    "        plt.subplot(1, nbColsPlot, topic%4 + 1)\n",
    "        plt.ylim(0, num_top_words + 0.5)\n",
    "        plt.xticks([]) \n",
    "        plt.yticks([])\n",
    "        plt.title('Topic #{}'.format(topic+1))\n",
    "        words = topics[topic][0]\n",
    "        scores = topics[topic][1]\n",
    "        for word in range(len(words)):\n",
    "            font_size = fontsize_init*scores[word]\n",
    "            font_size = min(font_size, MAGIC_NUMBER)\n",
    "            plt.text(0.05, num_top_words-word-0.5, vocabulary[words[word]], fontsize=font_size) \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_topics(topics, vocabulary, nbTopics=20, n_terms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the pipeline that we will follow for the LDA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizerModel' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d5a2eb4342bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcount_vectorizer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bbb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vectors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizerModel' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# random dataframe \n",
    "sentenceDataFrame = sqlContext.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\"),\n",
    "    (3, \"I want a coffee before going to bed \"),\n",
    "    (4, \"Today is a big day !!!\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "# tokenization\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "#with open(\"regexTokenized.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(regexTokenized, f)\n",
    "# remove stop words\n",
    "regexTokenized.save('aaa')\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "removed = remover.transform(regexTokenized)\n",
    "\n",
    "removed.save('ooo')\n",
    "#with open(\"removed_stopwords.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(removed, f)\n",
    "# create the tf-idf matrix\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "count_vectorizer_model = cv.fit(removed)\n",
    "tf = count_vectorizer_model.transform(removed)\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "with open(\"bbb.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocabulary, f)   \n",
    "\n",
    "idf = IDF(inputCol=\"vectors\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "tfidf.save('ccc')\n",
    "#with open(\"tfidf.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(tfidf, f)\n",
    "\n",
    "# initialize parameters\n",
    "nbTopics=3\n",
    "n_terms=3\n",
    "\n",
    "corpus = tfidf.select(F.col('id').cast(\"long\"), 'tfidf').rdd.map(lambda x: [x[0], x[1]])\n",
    "ldaModel = LDA.train(corpus, k=nbTopics)\n",
    "# extraction vocabulary\n",
    "\n",
    "\n",
    "ldaModel.save('ddd')\n",
    "#with open(\"vocabulary.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(ldaModel, f)    \n",
    "# extracting topics\n",
    "#topics = ldaModel.describeTopics(maxTermsPerTopic=n_terms)\n",
    "#with open(\"topics.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(topics, f)\n",
    "\n",
    "\n",
    "for topic in range(len(topics)):\n",
    "    print(\"topic {} : \".format(topic))\n",
    "    words = topics[topic][0]\n",
    "    scores = topics[topic][1]\n",
    "    for word in range(len(words)):\n",
    "        print(vocabulary[words[word]], \"-\", scores[word])\n",
    "        \n",
    "plot_topics(topics, vocabulary, nbTopics=3, n_terms=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sqlContext.createDataFrame('/user/khau/regexTokenized')\n",
    "a = sqlContext.read.load('/user/khau/regexTokenized')\n",
    "a.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Milestone 3: the data story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Play around with the parameters of the LDA algorithm in order to find the optimal values for α and β. \"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"repeat the same technique for spanish and french\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"algorithm for detecting the top trends\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"trying to find patterns between trends\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"analyse the data per user \n",
    "    ex. which topics he tweets most about ? \n",
    "        does it change over time ? \"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"find top users that were the most mentioned by somebody else\"\"\"\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"visualization of the result(per language, per month ...)\"\"\"\n",
    "\n",
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
